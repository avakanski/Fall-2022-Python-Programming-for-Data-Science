{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe026a0-1dad-40ef-a202-791e9ab53609",
   "metadata": {},
   "source": [
    "# Lecture 28 - Reproducible Data Science Projects <a id=\"section0\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c93718",
   "metadata": {},
   "source": [
    "- [28.1 Introduction](#section1)\n",
    "- [28.2 Docker](#section2)\n",
    "    - [28.2.1 Installing Docker](#section3)\n",
    "- [28.3 Hello World in Docker](#section4)\n",
    "- [28.4 Scikit-learn Models in Docker](#section5)\n",
    "- [28.5 TensorFlow-Keras Models in Docker](#section6)\n",
    "- [28.6 Docker Registries and Repositories](#section7)   \n",
    "- [28.7 Kubernetes](#section8)   \n",
    "- [References](#section9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bed47f",
   "metadata": {},
   "source": [
    "# 28.1 Introduction <a id=\"section1\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271810f7-6fc7-4f81-9349-7a0fea3ef7f8",
   "metadata": {},
   "source": [
    "Ensuring the reproducibility of Data Science projects is important for several reasons. One is that our customers (internal within your organization, or external) may use different operating systems, code dependencies, hardware, and there should be able to use our developed product and obtain the expected results. Another reason is collaborative development, where we may need to share our code with collaborators, and to ensure reproducibility requires information about the full environment in which the code was developed. \n",
    "\n",
    "Predictions of Machine Learning models depend on the model architecture and parameters, code, and libraries and dependencies needed to run the code. Ensuring reproducibility of models is a challenge that can affect the performance if not addressed. Regarding the model architecture and weights, we can simply save and load them, to ensure that the same model is used and that the predictions are consistent. Regarding code, we can use fixed random seeds and version control to ensure that the same data and code are used for training the model and making predictions. Dealing with code libraries and dependencies is more challenging, because libraries are updated frequently, and the model behavior can change, or even worse, the code can crash with some updates. To address this issue requires to apply a strategy for constraining the environment in which the model is developed and deployed.   \n",
    "\n",
    "The main strategies to constrain the libraries and dependencies include:\n",
    "1. Interoperable standards,\n",
    "2. Virtual environments,\n",
    "3. Docker containers. \n",
    "\n",
    "The first strategy involves standards like Open Neural Network Exchange (ONNX), which offers a format for neural networks which allows interoperability between different frameworks, such as PyTorch, TensorFlow, etc. Despite the potential, this strategy requires to frequently update the tools as the frameworks are updated, and it often has bugs in the translation layers. \n",
    "\n",
    "The second strategy is to use virtual environments in Python or conda. Although using virtual environments can constrain the environment, they provide only partial solution, since framework dependencies, such as hardware libraries, may be outside the scope of virtual environments and cannot be addressed with this strategy. \n",
    "\n",
    "The third strategy that relies on using Docker containers is widely adopted at present time. ***Docker*** is an open-source library that uses containers to constrain project dependencies. ***Container*** is a standardized unit of fully packaged software used for local development, shipping code, and deploying system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14537225-a068-4ebb-8cc8-c2983b309de8",
   "metadata": {},
   "source": [
    "# 28.2 Docker <a id=\"section2\"/>\n",
    "\n",
    "An example of the reproducibility challenge with Data Science projects is shown in the next figure, depicting the multiple points for failure when we move our code to a cluster. These can include different versions of libraries, dependencies, drivers, or operating system. The main advantages of Docker containers are that they contain all dependencies, including hardware libraries, to ensure reproducible and consistent code performance. \n",
    "\n",
    "<img style=\"float: center; height:320px;\" src=\"images/points_of_failure.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff3e56-25fe-4e4d-83e5-8ea168cf3c0e",
   "metadata": {},
   "source": [
    "Characteristics of containers are: \n",
    "- Portable, since the containers are detached from the underlying hardware and the platform that runs them. This allows to seamlessly port your code and projects from your local machine to other machines, e.g., that run on different operating systems or use different hardware, or to external clusters that offer GPU resources. Portability enables teams to easily collaborate on projects and experiment with new software and frameworks, without the need to spend time setting up the environment for running code. \n",
    "- Lightweight, since the containers include a minimal amount of data.\n",
    "- Secure, the exposed attack surface of a container is extremely small. \n",
    "- Facilitate deployment, where Data Science projects can be easily containerized and deployed to the end-users. \n",
    "\n",
    "Note also that containers are different from virtual machines. Virtual Machines require the hypervisor to virtualize the full hardware of the machine. Examples are AWS, Azure, or Google Cloud instances. Containers do not require hypervisor or hardware virtualization. They are isolated user-space environments, and are smaller in size and faster to boot. \n",
    "\n",
    "Important concepts for working with Docker include:\n",
    "- *Dockerfile* - defines how to build an image.\n",
    "- *Image* - is a built packaged environment.\n",
    "- *Container* - is where images are run.\n",
    "\n",
    "We will explain these concepts through several examples in the following sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d249b8-977c-434c-9222-b0fabce0223e",
   "metadata": {},
   "source": [
    "## 28.2.1 Installing Docker <a id=\"section3\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3502745d-4d43-4fad-9460-bce9ea56d629",
   "metadata": {},
   "source": [
    "Using Docker on a Windows machine can be done by installing [Docker Desktop for Windows](https://docs.docker.com/desktop/install/windows-install/). This requires to download the app and install it. Depending on the Windows operation system, it may also require to install updates for Windows Subsystem for Linux, and other libraries. It is also possible to install Docker from the command line. Please follow the instructions on the [Docker page](https://docs.docker.com/desktop/install/windows-install/) for more detailed information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fbf9c2",
   "metadata": {},
   "source": [
    "# 28.3 Hello World in Docker <a id=\"section4\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db493fb",
   "metadata": {},
   "source": [
    "Let's start with a simple \"Hello World\" example in Docker, where the goal is to create a container with a Python file to print the Hello World! statement.\n",
    "\n",
    "### Step 1: Create a Python Script\n",
    "\n",
    "For this purpose, we will create the Python script shown below that has just one line of code, and we will save it under the name `hello_world.py` in the folder `demo1`. \n",
    "\n",
    "<img style=\"float: center; height:70px;\" src=\"images/hello_world_script.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb57f1e-073e-4c3e-a7bd-d70fdb12652c",
   "metadata": {},
   "source": [
    "### Step 2: Create a Dockerfile\n",
    "\n",
    "**Dockerfiles** provide instructions for defining custom environments for our projects. I.e., for each particular project we will define the required software packages and libraries, and once the environment is specified in a Dockerfile, we will use the file to build a Docker image.  \n",
    "\n",
    "It is common to start the Dockerfilw with a *base environment*, which can simply include the latest Python version. Alternatively, as a base environment we can import pre-build environments by other developers. Docker provides access to *Docker Hub*, which is a registry with a large number of Docker images that have been published by other developers, and allows to directly import and take advantage of many pre-built environments.  \n",
    "\n",
    "Dockerfiles are plain text files, and for this simple project the Dockerfile is shown below: \n",
    "- The first line uses the `FROM` command to instruct Docker to use Python as a base image for the environment. The term `latest` instructs Docker to retrieve the image of the latest official Python version. Docker will first check if we have the latest Python version installed on our machine, and if we don't, Docker will automatically download the image from Docker Hub. Alternatively, if we wanted to work with an earlier Python version, we can just write `FROM python:3.5` or `FROM python:2.7`, for example. \n",
    "- The second line uses the `COPY` command to copy the created `hello_world.py` file from the local folder on my computer to the filesystem of the image. The notation `./` is used for the working directory of the image. The syntax is `COPY source_directory destination_directory`. \n",
    "- The third line uses the `CMD` command to execute the `hello_world.py` script from the working directory in the image when the image will be run. \n",
    "\n",
    "<img style=\"float: center; height:150px;\" src=\"images/dockerfile1.png\">\n",
    "\n",
    "The file needs to be saved under the name `Dockerfile`. \n",
    "The organization of the `Demo1` folder is:\n",
    "```\n",
    "Demo1\n",
    "    ├── Dockerfile\n",
    "    ├── hello_world.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4026ff-86ff-4587-bfc3-fafdfad43d42",
   "metadata": {},
   "source": [
    "### Step 3: Build the Docker Image\n",
    "\n",
    "***Docker image*** is a template that contains instructions for creating a container that can run on the Docker platform. It provides a convenient way to package up code and preconfigured environments, which we can use for our own private use or share publicly with other Docker users. \n",
    "\n",
    "To build a Docker image from the Dockerfile, we will use the following code.\n",
    "\n",
    "```\n",
    "docker build -t demo1 .\n",
    "```\n",
    "\n",
    "Docker code begins with the keyword `docker`. The command `build` specifies to build a Docker image. `-t` is used to set a tag to the image, and in this case we tag the image with the name `demo1`. The dot `.` at the end of the line specifies to build the image by using the Dockerfile that is located in the current directory where `hello_world.py` script is located. \n",
    "\n",
    "<img style=\"float: center; height:180px;\" src=\"images/build1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1671282a-7eaf-4bd5-b53a-a04bea68839e",
   "metadata": {},
   "source": [
    "The Docker image that we just built represents a collection of files that are required for an operational environment. Each of the files that make up a Docker image is referred to as a *layer*. The images can contain multiple application codes and dependencies, i.e., they can have multiple layers. \n",
    "\n",
    "In general, it is possible to build a Docker image by typing Docker commands interactively in the command line, and creating a Dockerfile is not required. However, Dockerfiles provide convenience and a documented record of the steps taken to assemble an image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5518fddc-9a06-4253-ad08-ca93be00b7af",
   "metadata": {},
   "source": [
    "### Step 4: Run the Docker Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740a48ab-f3a3-423f-bef7-42b9db6b99b9",
   "metadata": {},
   "source": [
    "To run the image, we just use the `run` command, followed by the image tag `demo1`. This will execute the `hello-world.py` script in the container, which will display `Hello World!` in the terminal.\n",
    "\n",
    "```\n",
    "docker run demo1\n",
    "```\n",
    "\n",
    "<img style=\"float: center; height:35px;\" src=\"images/run1.png\">\n",
    "\n",
    "In summary, we created a container that is independent of our local machine and that has an implementation of Python, and we displayed a message in that container. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fae923-ba0d-4b07-a5fa-aa036b9befb5",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The figure below shows the basic steps in Docker. We begin with a Dockerfile that contains the required commands to build a custom Docker image for our application. The Docker image is a custom environment, that contains the required code, libraries, and dependencies to ensure that the application that we developed will function as intended. Docker containers are run using the Docker image. In a container, an image provides all the required files to run the application. By using containers, applications are isolated from other processes, so that other processes cannot affect the current application.\n",
    "\n",
    "<img style=\"float: center; height:160px;\" src=\"images/img1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26fc3b7-6e0d-47ce-8509-78f8c4fe2b8c",
   "metadata": {},
   "source": [
    "Docker containers have an analogy with cargo containers used for shipping products. Once we place our product (code and dependencies) in the container, we can ship it by boat or train (laptop or cloud), and when it gets to its destination, it continues to function (run) just as before the shipment was made.\n",
    "\n",
    "And one more note is that Docker has an official `Hello World` image, which can be run directly from the terminal, and it is typically used by users when they install Docker for the first time to verify that it works properly. The following figure displays the output of the `Hello World` image.\n",
    "\n",
    "<img style=\"float: center; height:360px;\" src=\"images/docker_helloworld.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da0a44e-40a3-42d4-b46d-db01ed1b294d",
   "metadata": {},
   "source": [
    "# 28.4 Scikit-learn Models in Docker <a id=\"section5\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84255c90-9428-4371-9ead-44840bf396b4",
   "metadata": {},
   "source": [
    "In this section, we will learn how to create a container for running a scikit-learn model. \n",
    "\n",
    "To demonstrate the concept, we will use the *Boston Housing Dataset* that is available in scikit-learn. The dataset contains 506 records about housing prices in Boston, and it includes 12 features related to the number of rooms, air quality in the area, student-teacher ratio, crime rate, and similar information. The target column is the median value of the house. This is a regression problem, where the goal is for a given set of input features to predict the housing price. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b810046c-27c7-4bfa-ac34-aefbfa1fdd52",
   "metadata": {},
   "source": [
    "For this task, we will create a new folder, containing a `Dockerfile` to build the Docker image, `train.py` script for loading the dataset, training the model, and saving the model parameters, and `inference.py` script for loading the trained model and predicting on test instances. \n",
    "\n",
    "```\n",
    "Demo2-sklearn\n",
    "    ├── Dockerfile\n",
    "    ├── train.py\n",
    "    ├── inference.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9081d7-6ec0-4be3-9562-aaaccc78429a",
   "metadata": {},
   "source": [
    "### Step 1: Create `Train.py` script\n",
    "\n",
    "The code in `train.py` script is shown below (note that if you run the cell in this Jupyter notebook, it will give errors, because the code is intended to be executed as a script). \n",
    "\n",
    "This script is easy to understand. We import necessary libraries, load the data, fit a Gradient Boosting Model, and save the model. From the imported `joblib` package we used the `dump` method to serialize the model parameters and save them at the `MODEL_PATH` location. Also note that the `train.py` script defines the paths to the directory `MODEL_DIR` and saved model `MODEL_FILE`. This will allow to pass the path to Docker at build time, and embed these locations into the Docker image. Then, when the container is run, the script will read the locations of the files from the image, and use the saved model to predict on new data during inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2782a74b-0463-4d8d-8080-12c66aa5cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn import ensemble, datasets\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "from joblib import dump\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Directory paths \n",
    "MODEL_DIR = os.environ[\"MODEL_DIR\"]\n",
    "MODEL_FILE = os.environ[\"MODEL_FILE\"]\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE)\n",
    "\n",
    "# Load and split data\n",
    "print(\"Loading data...\")\n",
    "boston = datasets.load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data,  boston.target, random_state=13, test_size=0.1)\n",
    "\n",
    "# Fit regression model\n",
    "print(\"Fitting model...\")\n",
    "clf = ensemble.GradientBoostingRegressor()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "print(\"Saving model to: {}\".format(MODEL_PATH))\n",
    "dump(clf, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ca1eff-42c5-4c56-a154-73a1e00f2ab9",
   "metadata": {},
   "source": [
    "### Step 2 Create `Inference.py` script\n",
    "\n",
    "The script `inference.py` is very similar to `train.py`, except that the trained model is loaded, and in the last few lines of code, the loaded model is used to predict the price for the first 10 instances from the test dataset. The output includes both the predicted prices and the ground-truth prices from `y_test`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc48ad6-2ac4-4ff7-b978-3e93b5ddbf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from joblib import load\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Directory paths \n",
    "MODEL_DIR = os.environ[\"MODEL_DIR\"]\n",
    "MODEL_FILE = os.environ[\"MODEL_FILE\"]\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE)\n",
    "\n",
    "# Load and split data\n",
    "print(\"Loading data...\")\n",
    "boston = datasets.load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data,  boston.target, random_state=13, test_size=0.1)\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model from: {}\".format(MODEL_PATH))\n",
    "clf = load(MODEL_PATH)\n",
    "\n",
    "# Run inference\n",
    "print(\"Predicting...\")\n",
    "y_pred = clf.predict(X_test[:10])\n",
    "print(\"Predicted price by the model:\", np.around(y_pred,1))\n",
    "print(\"Ground-truth price:\", y_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf93ffd1-341e-4901-a74a-008bb653c2b9",
   "metadata": {},
   "source": [
    "### Step 3: Create `Dockerfile`\n",
    "\n",
    "The `Dockerfile` for this project is provided below, and contains the following parts:\n",
    "- `FROM jupyter/scipy-notebook` uses a pre-built image `jupyter/scipy-notebook` as a base image. The image contains a `python` installation, `scipy`, and other libraries to facilitate working with `Jupyter`. This image will be downloaded from Docker Hub. \n",
    "- The next lines are used to first create a new directory called `model` where the trained model will be saved. Afterward, 2 environment variables are defined for `MODEL_DIR` and `MODEL_FILE` that will reference the newly created directory and the name of the saved model. The directory `/home/jovyan/` was set in the imported base image `jupyter/scipy-notebook`.\n",
    "- The `COPY` commands are used to copy the `train.py` and `inference.py` scripts from our local machine to the working directory of the image. \n",
    "- `RUN python train.py` will execute the script to train and save the model. This will ensure that the model is saved at a specific location , and it is ready to predict on new data when the image will be run. An advantage is that if the model training fails, it will happen when we build the image rather than at run time, which allows to debug the issue. \n",
    "- `CMD [ \"python\", \"./inference.py\"]` will execute the `inference.py` script when we run the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d997e10e-1ac3-47a6-ae27-ec44f7c910ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM jupyter/scipy-notebook\n",
    "\n",
    "RUN mkdir model\n",
    "ENV MODEL_DIR=/home/jovyan/model\n",
    "ENV MODEL_FILE=clf.joblib\n",
    "\n",
    "COPY train.py ./train.py\n",
    "COPY inference.py ./inference.py\n",
    "\n",
    "RUN python train.py\n",
    "\n",
    "CMD [ \"python\", \"./inference.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98365da-205a-4238-8855-f7a99f00ab61",
   "metadata": {},
   "source": [
    "### Dockerfile Commands\n",
    "\n",
    "Common commands that are used in Dockerfiles are shown in the next table. \n",
    "\n",
    "|**Command**|**Purpose**|\n",
    "|--|--|\n",
    "|FROM|To specify the base image.|\n",
    "|WORKDIR|To set the working directory.|\n",
    "|RUN|To install any libraries and packages required for the container.|\n",
    "|COPY|To copy files or directories from a specific location.|\n",
    "|ADD|To add remote URLs and unpack compressed files.|\n",
    "|ENTRYPOINT|To specify commands to be executed when the container starts.|\n",
    "|CMD|Commands the container executes.|\n",
    "|EXPOSE|To define the port to access the container application.|\n",
    "|LABEL|To add metadata to the image.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e73075-5d7c-4150-98f4-b5353177af08",
   "metadata": {},
   "source": [
    "### Step 4: Build the Docker Image\n",
    "\n",
    "Next, we will build a Docker image from the Dockerfile, similarly to the previous example. Often, tags in the form `image_name:tag_name` are used, as in this case, where we assigned the image name `demo2-sklearn` and we assigned the tag name `0.1`. The tag name allows to apply versioning to Docker images, where, for instance, we can assign tag name `0.2` to the next version of image, or `1.0` if significant updates have been applied. If the tag name is omited, Docker will pull the most recent image version, which is identified by the tag `latest`. \n",
    "\n",
    "\n",
    "```\n",
    "docker build -t demo2-sklearn:0.1 .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf704b9-25df-4066-8aa5-a7fbbed76ef0",
   "metadata": {},
   "source": [
    "To list the Docker images that we have built, we can use, well, `docker images`. Note that the image `demo1` has the tag name `latest`, and the image `demo2-sklearn` has the tag name `0.1`.\n",
    "\n",
    "\n",
    "<img style=\"float: center; height:60px;\" src=\"images/images_list.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a2d3f4-0d8c-49f3-b791-c59514e4f356",
   "metadata": {},
   "source": [
    "Now that we have an image that contains a saved trained model, next, we will run a container to make predictions using the model. \n",
    "\n",
    "### Step 5: Run the Docker Image\n",
    "\n",
    "We run the image by using `docker run` and the name of the image. This will load the saved model, make predictions, and display the predicted house prices for the first 10 records in the test dataset. For comparison, the target prices for the first 10 records are also displayed. In a real-world case, the model will make predictions on new data samples (e.g., the price for house information that may be inputted by a realtor). \n",
    "\n",
    "```\n",
    "docker run demo2-sklearn:0.1\n",
    "```\n",
    "\n",
    "<img style=\"float: center; height:80px;\" src=\"images/run2.png\">\n",
    "\n",
    "Note as well that we can lunch multiple containers from one Docker image, with the containers maintaining their own individual state although sharing the same image. The changes to each individual container are stored in a *container layer*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff692dbe-1af6-495d-b49d-e44e94188098",
   "metadata": {},
   "source": [
    "### Docker Desktop\n",
    "\n",
    "Docker Desktop for Windows also provides lists of all created images, containers, and other related information. For instance, in the Containers section, we can see the two containers that we run `demo1` and `demo2-sklearn` with tag names `latest` and `0.1`, respectively. \n",
    "\n",
    "<img style=\"float: center; height:380px;\" src=\"images/docker_desktop.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd0e3c-6aeb-4a29-b2ad-a3a27b330501",
   "metadata": {},
   "source": [
    "# 28.5  TensorFlow-Keras Models in Docker <a id=\"section6\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32a415a-1885-4d29-b031-1f2a7bd9cff9",
   "metadata": {},
   "source": [
    "One more similar example follows, which uses TensforFlow-Keras libraries for creating a Conv Net model for predicting the digits in the MNIST dataset.\n",
    "\n",
    "The organization of the folder is as follows:\n",
    "\n",
    "```\n",
    "Demo3-mnist\n",
    "    ├── Dockerfile\n",
    "    ├── mnist_train.py\n",
    "    ├── mnist_inference.py\n",
    "    ├── requirements.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd334e-389e-4c46-b7e0-77104a226b2d",
   "metadata": {},
   "source": [
    "### Step 1: Create `mnist_train.py` script\n",
    "\n",
    "The script contains code for creating a Convolutional Neural Network model, compiling and training the model, and saving the model weights in a `cnn_model.h5` file using the HDF5 format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21cee7ee-de13-4419-bebd-f84f3e913c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 2us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 3s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 1s 0us/step\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 60s 32ms/step - loss: 0.4081 - accuracy: 0.8565\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 60s 32ms/step - loss: 0.2771 - accuracy: 0.8999\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 58s 31ms/step - loss: 0.2353 - accuracy: 0.9140\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 61s 32ms/step - loss: 0.2074 - accuracy: 0.9247\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 57s 30ms/step - loss: 0.1841 - accuracy: 0.9324\n"
     ]
    }
   ],
   "source": [
    "#importing modules\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "#Loading dataset\n",
    "mnist_fashion = keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images,test_labels) = mnist_fashion.load_data()\n",
    "\n",
    "# Scaling and reshaping\n",
    "training_images = training_images/255.0\n",
    "# training_images = training_images.reshape((60000,28,28,1))\n",
    "\n",
    "#Building layers of model\n",
    "cnn_model = keras.models.Sequential()\n",
    "cnn_model.add(keras.layers.Conv2D(50, (3, 3), activation='relu', input_shape=(28, 28, 1), name='Conv2D_layer'))  #\n",
    "cnn_model.add(keras.layers.MaxPooling2D((2, 2), name='Maxpooling_2D'))\n",
    "cnn_model.add(keras.layers.Flatten(name='Flatten'))\n",
    "cnn_model.add(keras.layers.Dense(50, activation='relu',name='Hidden_layer'))\n",
    "cnn_model.add(keras.layers.Dense(10, activation='softmax',name='Output_layer'))\n",
    "\n",
    "# compiling\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fitting the model\n",
    "cnn_model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "# saving the weights\n",
    "cnn_model.save_weights(\"cnn_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e2788-8727-48f3-afc4-066c93b4cb13",
   "metadata": {},
   "source": [
    "### Step 2 Create `mnist_inference.py` script\n",
    "\n",
    "The script `mnist_inference.py` is very similar, where the saved model is loaded, and it is used to predict the digits in the first 10 images in the test dataset. For comparison, the real labels for the 10 test images are displayed as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab98af43-1ef2-4337-b654-a1a50b20a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing modules\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "#Loading dataset\n",
    "mnist_fashion = keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images,test_labels) = mnist_fashion.load_data()\n",
    "\n",
    "# Scaling and reshaping\n",
    "test_images =test_images/255.0\n",
    "# test_images = test_images.reshape((10000,28,28,1))\n",
    "\n",
    "#Building layers of model\n",
    "cnn_model = keras.models.Sequential()\n",
    "cnn_model.add(keras.layers.Conv2D(50, (3, 3), activation='relu', input_shape=(28, 28, 1), name='Conv2D_layer'))  #\n",
    "cnn_model.add(keras.layers.MaxPooling2D((2, 2), name='Maxpooling_2D'))\n",
    "cnn_model.add(keras.layers.Flatten(name='Flatten'))\n",
    "cnn_model.add(keras.layers.Dense(50, activation='relu',name='Hidden_layer'))\n",
    "cnn_model.add(keras.layers.Dense(10, activation='softmax',name='Output_layer'))\n",
    "\n",
    "# compiling\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# loading the weights\n",
    "cnn_model.load_weights(\"cnn_model.h5\")\n",
    "\n",
    "# Predict\n",
    "print(\"Predict the digit in the first 10 images in the test dataset\")\n",
    "y_pred = cnn_model.predict(test_images[:10])\n",
    "print(\"Predicted image label:\", np.argmax(y_pred,1))\n",
    "print(\"Ground-truth image label:\", test_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7740fe82-a7be-4fa3-805b-3926972c3621",
   "metadata": {},
   "source": [
    "### Step 3: Create `Dockerfile`\n",
    "\n",
    "The `Dockerfile` is similar to the example in the previous section. It uses a Python 3.7.5 version, where `-slim` indicates to use a lightweight image of Python. This will reduce the size of the image, but it may require to install some additional libraries. Also note that the required libraries are provided in a `requirements.txt` file, instead of typing all required libraries individually in the Dockerfile. The rest of the lines copy the required files to the image, and then run the `requirements.txt` file to install the libraries in the image, and the `mnist_train.py` file to train the model and save it in the image.\n",
    "\n",
    "```\n",
    "numpy===1.21.5\n",
    "keras===2.9.0\n",
    "tensorflow===2.9.1\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c362034-ce42-4a8a-b563-8e4a33ea929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM python:3.7.5-slim\n",
    "\n",
    "COPY requirements.txt ./requirements.txt\n",
    "COPY mnist_train.py ./mnist_train.py\n",
    "COPY mnist_inference.py ./mnist_inference.py\n",
    "\n",
    "RUN pip install -r requirements.txt\n",
    "RUN python mnist_train.py\n",
    "\n",
    "CMD [ \"python\", \"./mnist_inference.py\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1543ccb-e7b8-4754-92d8-604af8627199",
   "metadata": {},
   "source": [
    "### Step 4: Build and Run the Docker Image\n",
    "\n",
    "Next, we will build the image `demo3-mnist` with a tag name `v1.0.0` and run it. The output is shown below. \n",
    "\n",
    "```\n",
    "docker image build -t demo3-mnist:v1.0.0 .\n",
    "\n",
    "docker run demo3-mnist:v1.0.0  \n",
    "```\n",
    "\n",
    "<img style=\"float: center; height:180px;\" src=\"images/run3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0960c63",
   "metadata": {},
   "source": [
    "# 28.6 Docker Registries and Repositories <a id=\"section7\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c658dd-047a-4740-927a-dd947026c86d",
   "metadata": {},
   "source": [
    "***Container repositories*** are the physical locations where Docker images are  stored. Each repository contains different versions of each image, where each image is referenced by a different tag. For example, on Docker Hub, `mysql` is a repository that contains different versions of the Docker image for `MySQL`.\n",
    "\n",
    "***Container registry*** is a set of repositories. The main types of container registries are:\n",
    "- Docker Hub: it is the official public container registry maintained by Docker, which hosts over 100,000 container images created by software vendors, open-source projects, and Docker’s community of users.  \n",
    "- Third-party registry services: are managed registries that allow users to store and share their Docker images. Examples of third-party registries include Azure Container Registry, Google Container Registry, Amazon ECR, Red Hat Quay, and JFrog Container Registry.\n",
    "- Self-hosted registries: are managed by organizations that prefer to host container images on their own on-premises infrastructure, typically due to security, compliance concerns, or lower latency requirements. Running self-hosted registries requires to deploy a registry server.\n",
    "\n",
    "### Pull and Push images on Docker Hub\n",
    "\n",
    "As we mentioned, Docker Hub allows users to host and manage their own images, where Docker images can be easily shared between collaborators, allowing every collaborator to use reproducible environment. \n",
    "\n",
    "To push built images to the Docker Hub registry, users will need to first log in to Docker Hub and then use the code to push the image. \n",
    "\n",
    "```\n",
    "docker login --username=your_username --password=your_password\n",
    "docker push username/image:tag\n",
    "```\n",
    "\n",
    "Also, pulling images by other developers from Docker Hub is very easy, and allows to use other images with little or no modification. \n",
    "\n",
    "```\n",
    "docker pull username/image:tag\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d343a-0241-4bac-9319-b071da2481fa",
   "metadata": {},
   "source": [
    "### Benefits of Docker\n",
    "\n",
    "- Code and dependencies can be packaged together into portable containers that can be run on a variety of different hosts, regardless of the hardware or operating system. \n",
    "- Provides versioning of build environments, where image versions can be tracked and rolled back.\n",
    "- Allow to replicate the working environment that is needed to train and deploy machine learning model on any system, and removes the reproducibility challenges. \n",
    "- Trained machine learning models can be easily wrapped into an API in a container, and deployed to remote servers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae98f02-eb57-40e3-80ce-d57013eecfca",
   "metadata": {},
   "source": [
    "# 28.7 Kubernetes <a id=\"section8\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46671e54-a50f-4a3a-8f97-e64976edccc1",
   "metadata": {},
   "source": [
    "Although Docker containers provide the tools for ensuring reproducibility, when there are multiple containers that need to be run on a system consisting of multiple machines, managing the containers can become difficult. ***Kubernetes*** is a platform for orchestration of containers, that manages the coordination and communication between multiple containers. That is, for a set of containers and a set of available machines on a cluster, Kubernetes determines the optimal assignment of machines for each container. This is advantageous because it allows the system to function without the need for human intervention. E.g., if one container shuts down unexpectedly, it will run another container to replace its function. \n",
    "\n",
    "The combination of Docker containers and Kubernetes orchestrator is well suited for developing and deploying *microservices*, where software applications are designed as a set of multiple, small, and decoupled services, that can be scaled and updated independently. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb56a71b-c447-4b4d-a432-be37e3205f35",
   "metadata": {},
   "source": [
    "### Pod\n",
    "\n",
    "The building block of Kubernetes is a **pod**, consisting of one or multiple related containers, a shared networking, and shared volume. The pods are designed as temporary objects, that will persist only for some period of time. Each pod is assigned a unique IP address to communicate with it. Volume is a storage location that is outside of a container, and allows to store data independently from the container. If a container is deleted, the volume is not deleted, and it can be used by other containers.\n",
    "\n",
    "<img style=\"float: center; height:200px;\" src=\"images/pod.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2e2884-5167-49c2-812b-2ed9982c60be",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "**Deployment** object consists of several pods, which include a *template* pod and multiple *replicas* pods. Having replicas of pods allows to replace any crashing pods at any time. \n",
    "\n",
    "<img style=\"float: center; height:260px;\" src=\"images/deployment.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52292e42-0539-4e2a-b588-e8613102fd0b",
   "metadata": {},
   "source": [
    "### Service\n",
    "\n",
    "**Service** object directs traffic to the pods, by balancing the incoming traffic between multiple pods. Also, if some pods crash, the service will launch new pods and it will direct the traffic to the new pods. Service also allows clients to access the pods without the need to know specific information about the pods (such as IP address or implementation). \n",
    "\n",
    "<img style=\"float: center; height:400px;\" src=\"images/service.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0bc6b6-f6a8-42e2-b2c6-e6826dc5ad21",
   "metadata": {},
   "source": [
    "### Ingress\n",
    "\n",
    "**Ingress** object controls which services that are exposed to external traffic. This allows to make certain services publicly available (e.g., predictions by a machine learning model) and keep certain services private (e.g., pods for training and updating the model with new data). In general, ingress refers to data entering a network, and egress is data leaving the network. \n",
    "\n",
    "<img style=\"float: center; height:500px;\" src=\"images/ingress.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb01ff0-9944-482d-8fd8-d761c8030558",
   "metadata": {},
   "source": [
    "### Control Plane\n",
    "\n",
    "Kubernetes **control plane** controls how workloads are executed, monitored, and maintained on a cluster. A figure showing the elements in a control plane is shown below. \n",
    "\n",
    "The cluster has a *master node* and multiple *worker nodes* that run the applications. For instance, a cluster can have several GPU-optimized worker nodes for model training and CPU-optimized worker nodes for making predictions.\n",
    "\n",
    "The master node includes: scheduler - for assigning objects to available machines, controller manager - monitors the state of the cluster and compares the current state to the desired state, API server - communicates with a client and accepts requests, and  etcd component - stores the current state of the cluster.  \n",
    "\n",
    "Each worked node includes a control component consisting of a kubelet and kube-proxy. The kubelet communicates with the API server to find out which workloads have been assigned to the node, and runs the pods to complete the assigned workloads. Kube-proxy enables containers from different pods to communicate with each other.\n",
    "\n",
    "<img style=\"float: center; height:500px;\" src=\"images/control_plane.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa0d697-d516-4e39-bd77-3a91dea4174b",
   "metadata": {},
   "source": [
    "The Kubernetes system is designed to handle any failures in the cluster. For example, if the master node is down, the applications would not be immediately affected, and a new master node will be launched to continue the operation of the system. \n",
    "\n",
    "Kubernetes are designed to orchestrate large number of workloads that may include multiple Data Science projects, distributed across clusters of servers, and serving continuously a large number of end-users. It is not the optimal solution for working with a single machine to run a lightweight load that does not need frequent changes and updates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389368f9",
   "metadata": {},
   "source": [
    "# References <a id=\"section9\"/>\n",
    "\n",
    "1. Full Stack Deep Learning course, Lecture 11: Deployment & Monitoring, available at [https://fullstackdeeplearning.com/spring2021/lecture-11/](https://fullstackdeeplearning.com/spring2021/lecture-11/).\n",
    "2. ML in Production, Docker for Machine Learning, by Luigi Patruno, available at: [https://mlinproduction.com/docker-for-ml-part-1/](https://mlinproduction.com/docker-for-ml-part-1/).\n",
    "3. A Beginner’s Guide to Understanding and Building Docker Images, available at: [https://jfrog.com/knowledge-base/a-beginners-guide-to-understanding-and-building-docker-images/](https://jfrog.com/knowledge-base/a-beginners-guide-to-understanding-and-building-docker-images/).\n",
    "4. Machine Learning with Docker, Demonstrations about Using Docker for Machine Learning, available at: [https://github.com/BINPIPE/docker-for-ml](https://github.com/BINPIPE/docker-for-ml).\n",
    "5. An introduction to Kubernetes, by Jeremy Jordan, available at: [https://www.jeremyjordan.me/kubernetes/](https://www.jeremyjordan.me/kubernetes/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e12fc40",
   "metadata": {},
   "source": [
    "[BACK TO TOP](#section0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
