{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPG5INqg-_qn"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/avakanski/Fall-2022-Python-Programming-for-Data-Science/blob/main/Lectures/Theme%203%20-%20Model%20Engineering%20Pipelines/Lecture%2018%20-%20Natural%20Language%20Processing/Lecture%2018%20-%20Natural%20Language%20Processing.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JhHIQqNe4Qs"
      },
      "source": [
        "<a name='section0'></a>\n",
        "# Lecture 18 Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEkmemKte4Qv"
      },
      "source": [
        "- [18.1 Introduction to Natural Language Processing (NLP)](#section1)\n",
        "- [18.2 Preprocessing Text Data](#section2)\n",
        "- [18.3 Text Tokenization with Keras Tokenizer](#section3)\n",
        "    - [18.3.1 Character-level Tokens](#section3-1)\n",
        "    - [18.3.2 Word-level Tokens](#section3-2)\n",
        "    - [18.3.3 Padding Word Sequences](#section3-3)\n",
        "- [18.4 Representation of Groups of Words in ML Models](#section4)\n",
        "- [18.5 Sequence Model Approach](#section5)\n",
        "    - [18.5.1 Word Embeddings](#section5-1)\n",
        "    - [18.5.2 Using TextVectorization Layer to Preprocess Text](#section5-2)\n",
        "    - [18.5.3 Sequence Modeling with Recurrent Neural Networks](#section5-3)\n",
        "- [References](#section6)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTFgm_bJe4Qw"
      },
      "source": [
        "<a name='section1'></a>\n",
        "\n",
        "# 18.1 Introduction to Natural Language Processing (NLP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFMFGCdwiw5k"
      },
      "source": [
        "***Natural Language Processing (NLP)*** is a branch of computer science (and more broadly, a branch of artificial intelligence) that is concerned with providing computers with the ability to understand texts and human language. \n",
        "\n",
        "Common tasks in NLP include:\n",
        "\n",
        "- *Text classification* — assign a class label to text based on the topic discussed in the text, e.g., sentiment analysis (positive or negative movie review), spam detection, content filtering (detect abusive content).\n",
        "- *Text summarization/reading comprehension* — summarize a long input document with a shorter text.\n",
        "- *Speech recognition* — convert spoken language to text.\n",
        "- *Machine translation* — convert text in a source language to a target language.\n",
        "- *Part of Speech (PoS) tagging* — mark up words in text as nouns, verbs, adverbs, etc. \n",
        "- *Question answering* — output an answer to an input question.\n",
        "- *Dialog generation* — generate the next reply in a conversation given the history of the conversation.\n",
        "- *Text generation/language modeling* — generate text to complete the sentence or to complete the paragraph.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF5bogcC7mUa"
      },
      "source": [
        "<a name='section2'></a>\n",
        "\n",
        "# 18.2 Preprocessing Text Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWMSWyTqjNym"
      },
      "source": [
        "In order to perform operations with text data, they first need to be converted into a numerical representation. \n",
        "\n",
        "Converting text data into numerical form for processing by ML models typically involves the following steps:\n",
        "- *Standardization* - remove punctuation, convert the text to lowercase.\n",
        "- *Tokenization* - break up the text into tokens (e.g., tokens can be individual words, several consecutive words (N-grams), or individual characters).\n",
        "- *Indexing* - assign a numerical index to each token in the training set (i.e., vocabulary).\n",
        "- Optional step: *Embedding* - assign a numerical vector to each index (e.g., one-hot encoding or word-embedding using embedding models such as word2vec, or GloVe).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc6LF4pHjNyn"
      },
      "source": [
        "### Text Standardization\n",
        "\n",
        "***Text standardization*** usually includes some or all of the following steps, depending on the application:\n",
        "- Remove punctuation marks (such as comma, period) or non-alphabetic characters (@, #, {, ]).\n",
        "- Change all words to lower-case letters, since the model should consider *Text* and *text* as the same word.\n",
        "\n",
        "Some NLP tasks can apply additional steps, such as:\n",
        "- Correct spelling errors or replace abbreviations with full words. \n",
        "- Remove stop words, such as *for*, *the*, *is*, *to*, *some*, etc.; if the task is text classification, these words are not relevant to the meaning of the text. \n",
        "- Apply stemming and lemmatization, which transforms words to their base form, such as changing the word *changing* to *change*, or *grilled* to *grill* since they have a common root. \n",
        "\n",
        "Applying text standardization is helpful for training machine learning models, because the models do not need to consider *Text* and *text* as two different words, which reduces the requirements for large training datasets. However, depending on the application, text standardization may remove information that can be important for some tasks, and this should always be considered when performing text preprocessing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7RF_VR3jNyo"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "***Tokenization*** is breaking up the text into a sequence of representative symbols called tokens. A *token* is simply a piece of data that stands in for another, more valuable piece of information. \n",
        "\n",
        "Tokenization can be performed at different levels:\n",
        "- *Character-level tokenization* - where each character is a token, and it is represented by a unique number. One of the traditional character encoding techniques is ASCII (American Standard Code Information Interchange). ASCII allows to convert any character to a numeric token. One disadvantage of this type of tokenization is that antigrams (words with same letters in different order, such as *silent* and *listen*) can have the same encoding, which can affect the performance of machine learning models. Consequently, character-level tokenization is not widely used in practice.\n",
        "- *Word-level tokenization* - where each word is a token, represented with a unique number. This type of encoding works is the most commonly used. \n",
        "- *N-gram tokenization* - where N consecutive words represent a token. For instance, N-grams consisting of two adjacent words are called bigrams, or three words constitute a trigram, etc. N-grams tokens preserve the words order and can potentially capture more information in the text. For instance, for spam filtering using  bigram tokens such as *mailing list* or *bank account* may provide more helpful information than using word-level tokens. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbD7810XjNyp"
      },
      "source": [
        "An example of text standardization and word-level tokenization is shown in the next figure.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/avakanski/Fall-2022-Python-Programming-for-Data-Science/main/Lectures/Theme%203%20-%20Model%20Engineering%20Pipelines/Lecture%2018%20-%20Natural%20Language%20Processing/images/tokenization.png' width=500px/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2SDR8h32fE9"
      },
      "source": [
        "<a name='section3'></a>\n",
        "\n",
        "# 18.3 Text Tokenization with Keras Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tDuayp80W1M"
      },
      "source": [
        "Keras provides a text preprocessing function `Tokenizer` for converting raw text into sequences of tokens. The `Tokenizer` performs both text standardization and tokenization. \n",
        "\n",
        "The Kears `Tokenizer` has the following arguments:\n",
        "- *num_words*: the maximum number of words to keep in the input text. It is better to set a high number if we are not sure, because if we set a number less than the words in the text, some words will not be tokenized.\n",
        "- *filters*: by default, all punctuations and special characters in the text will be removed. If we want to change that, we can provide a list of punctuations and characters to keep. \n",
        "- *lower*: can be True or False. By default, it is True, and that means all texts will be converted to lowercase.\n",
        "- *split*: separator for splitting words. A default separator is a space (\" \").  \n",
        "- *char_level*: can be True or False. By default, it is False and will perform word-level tokenization. If it is True, the function will perform character-level tokenization. \n",
        "- *oov_token*: oov stands for Out Of Vocabulary, and it denotes a symbol that will be added to the word_index to replace words that are not present in the input text. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hA9dZFJejNys"
      },
      "source": [
        "<a name='section3-1'></a>\n",
        "\n",
        "### 18.3.1 Character-level Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP3igoipEMDW"
      },
      "source": [
        "To use the `Tokenizer` for character-level tokenization, we need to set `char_level` to `True`. Let's set the number of tokens to 1,000. \n",
        "\n",
        "Let's apply it to the following simple sentence by using  the method `fit_on_texts()`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "alBwiPau5LC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd4286b2-0f15-43fa-ca8c-d58343877ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version:2.9.2\n",
            "Keras version:2.9.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "# Print the version of tf\n",
        "print(\"TensorFlow version:{}\".format(tf.__version__))\n",
        "print(\"Keras version:{}\".format(keras.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CNUzPUWIDfGi"
      },
      "outputs": [],
      "source": [
        "# A sample sentence\n",
        "sentence = ['TensorFlow is a Machine Learning framework']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vr496JvAEEIN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=1000, char_level=True)\n",
        "\n",
        "# Fitting tokenizer on sentences\n",
        "tokenizer.fit_on_texts(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_n39kkPvGQV7"
      },
      "source": [
        "When the `Tokenizer` separates the characters in text, it creates a  dictionary that maps each character to its token. We can inspect the dictionary by using the attribute `word_index`, although since we have set `char_level` to `True` in this case it is the character index. \n",
        "\n",
        "Note that the tokens start at 1.  By default, all letters are converted to lowercase. The first token is an empty space `' '`, the second is the letter `'e'`, etc. There are 17 unique characters in the sentence, including the empty space. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7ztDhDIGPuo",
        "outputId": "5c189c61-cd54-4c6d-caa3-da4dc9634c45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' ': 1, 'e': 2, 'n': 3, 'r': 4, 'a': 5, 'o': 6, 'i': 7, 's': 8, 'f': 9, 'l': 10, 'w': 11, 'm': 12, 't': 13, 'c': 14, 'h': 15, 'g': 16, 'k': 17}\n"
          ]
        }
      ],
      "source": [
        "char_index = tokenizer.word_index\n",
        "print(char_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2L349BGjNyu"
      },
      "source": [
        "The method `text_to_sequences` outputs the tokens for the text. You can check that the word `TensorFlow` has the tokens 13, 2, 3, 8, 6, 4, 9, 10, 6, 11, where each of the tokens corresponds to the letters listed in the `char_index`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAPxvWUljNyv",
        "outputId": "85b79d50-d413-467c-df9f-259efaadb514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[13, 2, 3, 8, 6, 4, 9, 10, 6, 11, 1, 7, 8, 1, 5, 1, 12, 5, 14, 15, 7, 3, 2, 1, 10, 2, 5, 4, 3, 7, 3, 16, 1, 9, 4, 5, 12, 2, 11, 6, 4, 17]]\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.texts_to_sequences(sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzhZW63ljNyv"
      },
      "source": [
        "*As* we mentioned earlier, character-level tokenization is rarely used, because it is challenging to deal with words that have the same characters in different order (antigrams) since they have the same tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkfbQe9XjNyv"
      },
      "source": [
        "<a name='section3-2'></a>\n",
        "\n",
        "### 18.3.2 Word-level Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uemMRwYrGwpb"
      },
      "source": [
        "To use the `Tokenizer` for tokenizing words instead of characters, we need to just change the argument `char_level` to `False`, which is the default setting, so we may as well just omit it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CmuIbnlKjNyv"
      },
      "outputs": [],
      "source": [
        "# Sample sentences\n",
        "sentences = ['TensorFlow is a Machine Learning framework.',\n",
        "             'Keras is a well designed deep learning API!',\n",
        "             'Keras is built on top of TensorFlow!']    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk1fTO2yjNyw"
      },
      "source": [
        "After the text is broken down into individual words, the `Tokenizer` builds a *vocabulary* of all words that are found in the input text, and assigns a unique integer to each word in the vocabulary. We can inspect the words by using again the attribute `word_index`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpIE4S3PHCbY",
        "outputId": "36aa8b25-0ab9-4a78-bc39-0d9f8a61a0be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'is': 1, 'tensorflow': 2, 'a': 3, 'learning': 4, 'keras': 5, 'machine': 6, 'framework': 7, 'well': 8, 'designed': 9, 'deep': 10, 'api': 11, 'built': 12, 'on': 13, 'top': 14, 'of': 15}\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer(num_words=1000)\n",
        "\n",
        "# Fitting tokenizer on sentences\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGmvr6I2GwwJ"
      },
      "source": [
        "There are 15 unique words in the above sentences. By default, all punctuations are removed and all letters are converted to lowercase. \n",
        "\n",
        "The tokens for the above three sentences are shown below. For instance, the first list `[2, 1, 3, 6, 4, 7]` represents the first sentence in the text `TensorFlow is a Machine Learning framework`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Xz_E4PXjNyx",
        "outputId": "c7caedc7-b5c6-4c60-fb1a-9429568d79b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2, 1, 3, 6, 4, 7], [5, 1, 3, 8, 9, 10, 4, 11], [5, 1, 12, 13, 14, 15, 2]]\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.texts_to_sequences(sentences))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAJYp5sAIstj"
      },
      "source": [
        "Also, `word_counts` can return the number of times each word appears in the sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOb-6DGTIIDp",
        "outputId": "243a907b-a807-4bc0-af06-85ceebf510ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('tensorflow', 2),\n",
              "             ('is', 3),\n",
              "             ('a', 2),\n",
              "             ('machine', 1),\n",
              "             ('learning', 2),\n",
              "             ('framework', 1),\n",
              "             ('keras', 2),\n",
              "             ('well', 1),\n",
              "             ('designed', 1),\n",
              "             ('deep', 1),\n",
              "             ('api', 1),\n",
              "             ('built', 1),\n",
              "             ('on', 1),\n",
              "             ('top', 1),\n",
              "             ('of', 1)])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "word_counts = tokenizer.word_counts\n",
        "word_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZE0QY8NjNyx"
      },
      "source": [
        "### Out of Vocabulary Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4832s6thIYV"
      },
      "source": [
        "To handle the case when the Tokenizer is applied to text that contains words that were not present in the original documents, we can define a special token `oov_token`. This token will be used to replace these words that are Out Of Vocabulary (OOV).\n",
        "\n",
        "In the example below, we set the `oov_token`, which has been assigned the token `1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHy4egiNh1dg",
        "outputId": "e7b8f6b7-5862-4fd5-9c5a-4e9ef52f416c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Word Out of Vocab': 1, 'is': 2, 'tensorflow': 3, 'a': 4, 'learning': 5, 'keras': 6, 'machine': 7, 'framework': 8, 'well': 9, 'designed': 10, 'deep': 11, 'api': 12, 'built': 13, 'on': 14, 'top': 15, 'of': 16}\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer(num_words=1000, oov_token='Word Out of Vocab')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tT78VLKjNyy",
        "outputId": "2df160b3-2d05-439e-e5d2-99ce51b66bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3, 2, 4, 7, 5, 8], [6, 2, 4, 9, 10, 11, 5, 12], [6, 2, 13, 14, 15, 16, 3]]\n"
          ]
        }
      ],
      "source": [
        "# Converting text to sequences\n",
        "print(tokenizer.texts_to_sequences(sentences))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bnw7dUui5Kv"
      },
      "source": [
        "Next, if we pass text with new words that the tokenizer was not fit to, the new words will be replaced with the `oov_token`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7mHGlEjjOG_",
        "outputId": "3d45f6be-5988-4577-cd8d-2f44f8407c66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 1, 3], [6, 2, 4, 1, 11, 5, 12]]\n"
          ]
        }
      ],
      "source": [
        "new_sentences = ['I like TensorFlow', # I and like are new words\n",
        "                'Keras is a superb deep learning API'] # superb is a new word \n",
        "\n",
        "print(tokenizer.texts_to_sequences(new_sentences))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj4rtaDbjNyz"
      },
      "source": [
        "And also, if we work with a large dataset of documents, we can limit the number of words in the vocabulary to 20,000 or 30,000, and consider the rare words as out-of-vocabulary words. This can reduce the feature space of the model, by ignoring those words that are present only once or twice in the large database."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt7W6Gdflok5"
      },
      "source": [
        "<a name='section3-3'></a>\n",
        "\n",
        "### 18.3.3 Padding Word Sequences "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUfBkgJimHpo"
      },
      "source": [
        "Most machine learning models require the input samples to have the same length/size. In Keras, the function `pad_sequences()` can be used to pad the text sequences with predefined values, so that they have the same length. \n",
        "\n",
        "The function `pad_sequences()` accepts the following arguments:\n",
        "- *sequence*: a list of sequences in integer forms (tokenized texts).\n",
        "- *maxlen*: maximum length of all sequences; if not provided, sequences will be padded to the length of the longest sequence.\n",
        "- *padding*: 'pre' (default) or 'post', whether to pad before the sequence or after the sequence.\n",
        "- *truncating*: 'pre' (default) or post', whether to remove the values from sequences larger than maxlen at the beginning or at the end of the sequences.\n",
        "- *value*: a float or a string to use as a padding value. By default, the sequences are padded with 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "print(tokenized_sentences)"
      ],
      "metadata": {
        "id": "-gtdBQdVRHEc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc64c84a-754f-4269-dae6-b81547aa71a3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3, 2, 4, 7, 5, 8], [6, 2, 4, 9, 10, 11, 5, 12], [6, 2, 13, 14, 15, 16, 3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgLaHzlYrf3j"
      },
      "source": [
        "The next cell shows the above sequences pre-padded with 0 to sequences with length 10. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i5cMbLQrthF",
        "outputId": "deaadad7-e5a7-4655-8034-e6235b39fdff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  0  3  2  4  7  5  8]\n",
            " [ 0  0  6  2  4  9 10 11  5 12]\n",
            " [ 0  0  0  6  2 13 14 15 16  3]]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "padded_sequences = pad_sequences(tokenized_sentences, maxlen=10)\n",
        "\n",
        "print(padded_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='section4'></a>\n",
        "\n",
        "# 18.4 Representation of Groups of Words in ML Models"
      ],
      "metadata": {
        "id": "OdhmWKLAooyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Representation of groups of words in machine learning models for text processing includes two categories of approaches:\n",
        "- ***Set models*** approach, the text is represented as unordered collection of words. Representatives of this group is the *bag-of-words* model.\n",
        "- ***Sequence models*** approach, where the text is represented as ordered sequences of words. These methods preserve the order of the words in the text. Representatives of this group are Recurrent Neural Networks, and Transformer Networks. \n",
        "\n",
        "The order of words in natural language is not necessarily fixed, and sentences with different orders of the words can have the same meaning. Also, different languages use different ways to order the words. As a result, defining the order of the words in text in NLP tasks is not straightforward. \n",
        "\n",
        "Bag-of-words models discard the information about the order of the words, where the term *bag* implies that the structure of the text is lost. A depiction of a bag-of-words is shown below, where the initial text is separated into word-level tokens, and the bag is created that includes all words. Also, instead of individual words, these models often employ N-gram representations. This type of models typically consider the frequency of occurrence of each word in the training data, and a classifier is trained by using the word counts as inputs. \n",
        "\n",
        "For instance, to create a spam filtering classifier, two bags-of-words can be created from the words in spam and non-spam emails. Presumably, the spam bad will contain trigger words (such as cheap, buy, stock) more frequently than the non-spam emails. A classifier will be trained using the two bags-of-words, and learn to differentiate trigger words from regular words. After the training, the classifier will analyze the words in the message, and predict the probability that these words belong to the spam or non-spam bag-of-words.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "acxXXFMPopRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://raw.githubusercontent.com/avakanski/Fall-2022-Python-Programming-for-Data-Science/main/Lectures/Theme%203%20-%20Model%20Engineering%20Pipelines/Lecture%2018%20-%20Natural%20Language%20Processing/images/bag_of_words.png' width=600px/>\n"
      ],
      "metadata": {
        "id": "4qBsT78wDv2O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The early applications of machine learning in NLP relied on bag-of-words models. Modern applications, especially those related to large language models, rely predominantly on sequence models. Before 2020, Recurrent Neural Networks were the preferred models for NLP applications. In recent years, Transformer Networks have replaced Recurrent Neural Networks as more powerful models for NLP tasks. "
      ],
      "metadata": {
        "id": "GGmSvyoAEvHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='section5'></a>\n",
        "\n",
        "# 18.5 Sequence Model Approach"
      ],
      "metadata": {
        "id": "iJ2D6GsMsAiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Sequence models*** preserve the order of words in the input text. Typical application of sequence models includes representing the words in text data with integer indices, mapping the integers to vector representations, and passing the vectors to a machine learning model, where the layers in the model will account for the ordering of input vectors.\n",
        "\n",
        "The input vectors to sequence models can be in the form of:\n",
        "- One-hot word vectors representation, or\n",
        "- Word embeddings representation."
      ],
      "metadata": {
        "id": "R-6vSeeksNq-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-Hot Word Vectors Representation"
      ],
      "metadata": {
        "id": "6fGeH8L8t67N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFjNamLou4pe"
      },
      "source": [
        "***One-hot word vectors*** representation is similar to encoding categorical features with one-hot encoding matrix. That is, the tokens for each word are converted to one-hot vector, having `1(hot)` for that word and `0(cold)` for all other words. An example is shown in the next figure, where we created a zero vector with length of 4, and assigned 1 for the index that corresponds to  every word. \n",
        "\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/avakanski/Fall-2022-Python-Programming-for-Data-Science/main/Lectures/Theme%203%20-%20Model%20Engineering%20Pipelines/Lecture%2018%20-%20Natural%20Language%20Processing/images/one_hot_encodding.png' width=300px/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGcbgsmjyho6"
      },
      "source": [
        "One-hot word vector representation is not an efficient way to represent text, because for large text datasets the input vectors can become quite large. For instance, a training set with 20,000 words will need to use one-hot vectors of size 20,000 to represent each word, and this results in slow training, as well as this type of word representation takes a lot of memory space.\n",
        "\n",
        "Using word embeddings is more efficient, and most modern NLP models rely on word embeddings for word representations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='section5-1'></a>\n",
        "\n",
        "## 18.5.1 Word Embeddings"
      ],
      "metadata": {
        "id": "7A4VE9IqyAFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Word embeddings*** representation is used to convert each word into a feature vector, in such as way that feature vectors of words that have similar semantic meaning will have close spatial positions in the embeddings space. \n",
        "\n",
        "The feature vectors represent the position of words in a higher-dimensional space. As a distance metric for calculating the distance between the vectors in the embedding space, often the cosine distance is used. For two vectors $u$ and $v$, *cosine similarity* is calculated as the dot (scalar) product of the vectors divided by the norm of the vectors, i.e., $\\dfrac{u\\cdot v}{||u||\\cdot ||v||}$. This way, the spatial distance between vectors is dependent on their semantic meaning.\n",
        "\n",
        "Two popular techniques for generating word embeddings are *word2vec* and *Glove*. These methods use neural networks to learn word embeddings from a large corpus of text. The feature vectors learned by these techniques can be imported as pretrained embeddings and applied to downstream tasks with smaller training datasets. \n",
        "\n",
        "Typical word embeddings are vectors in 256 to 1,024 dimensions. \n",
        "\n",
        "A simple example of word embedddings is shown below, where similar words are positioned closer to each other. \n",
        "\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/avakanski/Fall-2022-Python-Programming-for-Data-Science/main/Lectures/Theme%203%20-%20Model%20Engineering%20Pipelines/Lecture%2018%20-%20Natural%20Language%20Processing/images/word_embeddings.png' width=700px/>\n",
        "\n",
        "\n",
        "For example, the website  [Embedding Projector](http://projector.tensorflow.org) provides visualizations of word embeddings, and for an entered word displays other words that are adjacent in the embedding space. \n",
        "\n",
        "To demonstrate the use of word embedding with Keras, we will implement it for a sentiment analysis task, to classify movie reviews using the IMDB Reviews dataset. \n"
      ],
      "metadata": {
        "id": "rI04lGrByAZH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpTdRWWgzs0p"
      },
      "source": [
        "### Loading the IMDB Reviews Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMO8oqNCzwsL"
      },
      "source": [
        "IMDB Reviews Dataset can be downloaded from the built-in datasets in Keras.  There are 25,000 samples of movie reviews for training and 25,000 samples for validation. Setting `max_features` to 20,000 means we are only considering the first 20,000 words and the rest of the words will have the out-of-vocabulary token. Each movie review has a positive or negative label. \n",
        "\n",
        "The training and validation datasets will be loaded as lists with 25,000 elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hvfoy6M82QI9"
      },
      "outputs": [],
      "source": [
        "max_features = 20000\n",
        "\n",
        "(train_data, train_labels), (val_data, val_labels) = keras.datasets.imdb.load_data(num_words=max_features)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data))\n",
        "print(len(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx-Hxo8m_oc4",
        "outputId": "22c706aa-24ee-47f5-dd6d-9aece48c4925"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000\n",
            "25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displayed below is the third movie review. It is also a list, it contains 141 words, and as we can see the words in the dataset are already converted to tokens. "
      ],
      "metadata": {
        "id": "-DmqzQ-eAU1e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXKWTG9Pai4C",
        "outputId": "5c6bb9fc-fbd7-46e9-9078-7782353f3e7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in the third review 141\n",
            "[1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]\n"
          ]
        }
      ],
      "source": [
        "# Display the third movie review\n",
        "print('Number of words in the third review', len(train_data[2]))\n",
        "print(train_data[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Prz3AruWmAb5",
        "outputId": "01c47607-cfbd-4b36-f785-bacdd40a1871"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 0, 0, 1, 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Display the first 10 labels\n",
        "train_labels[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjd3rQSVaMhR"
      },
      "source": [
        "### Preparing the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgqbiz6p2liy"
      },
      "source": [
        "Let's pad the data using the `pad_sequences` function in Keras. Setting `maxlen` indicates to use the first 200 words in each movie review, and ignore the rest. Most movie reviews in the dataset are shorter than 200 words, however for those that are longer than 200 words some information will be lost. That is a tradeoff between computational expense and model performance.\n",
        "\n",
        "We can see in the next cell that for the third reivew, which has a length of 141 words, the first 59 words are now 0, and the length is 200."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "R85gam_ua92K"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "train_data = pad_sequences(train_data, maxlen=200)\n",
        "val_data = pad_sequences(val_data, maxlen=200)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the third movie review\n",
        "print('Shape of the third padded review:', train_data.shape, '\\n')\n",
        "print(train_data[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqHhsOm5BhtH",
        "outputId": "9b62f1c6-1f7e-4362-de1b-727348124011"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the third padded review: (25000, 200) \n",
            "\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    1   14   47    8   30   31    7    4  249  108    7\n",
            "    4 5974   54   61  369   13   71  149   14   22  112    4 2401  311\n",
            "   12   16 3711   33   75   43 1829  296    4   86  320   35  534   19\n",
            "  263 4821 1301    4 1873   33   89   78   12   66   16    4  360    7\n",
            "    4   58  316  334   11    4 1716   43  645  662    8  257   85 1200\n",
            "   42 1228 2578   83   68 3912   15   36  165 1539  278   36   69    2\n",
            "  780    8  106   14 6905 1338   18    6   22   12  215   28  610   40\n",
            "    6   87  326   23 2300   21   23   22   12  272   40   57   31   11\n",
            "    4   22   47    6 2307   51    9  170   23  595  116  595 1352   13\n",
            "  191   79  638   89    2   14    9    8  106  607  624   35  534    6\n",
            "  227    7  129  113]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Layer in Keras\n",
        "\n",
        "Keras has `Embedding` layer, which we will use to project the input tokens into a vector into an embedding space. The `Embedding` layer requires at the minimum to specify the number of possible tokens in the data sequences, and the dimensionality of the vectors in the embeddings space. The layer takes integer indices as inputs, and outputs a feature vector. It can be considered as a look-up table, which maps a vector to each integer.\n",
        "\n",
        "To understand how the Embedding layer works, let's take data with the maximum number of words set to 100, and represent them with 5-dimensional vectors. The Embedding layer will represent words with feature vectors of a given dimension. In the cell below, the layer just assigned random values to the numbers 1, 2, and 3, and we can see that to each number a 5-dimensional vector is assigned. However, the embeddings are trainable, and when we include the Embedding layer in a model, as we train the model, words that are similar will get closer in the embedding space."
      ],
      "metadata": {
        "id": "qlyWGixEViI_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xIZ5fkxQrB8",
        "outputId": "49854475-3adc-4e21-a692-8541a4de6356"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.03479839,  0.00630057,  0.02763673,  0.04852015, -0.02466449],\n",
              "       [-0.03284581, -0.00977626,  0.0160458 ,  0.03024981,  0.00200093],\n",
              "       [-0.02504357,  0.03114765, -0.02766613, -0.04830622,  0.03535182]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# embedding layer: represent a dataset with a vocabulary of 100 words with 5 dimensional vectors\n",
        "embedding_layer = tf.keras.layers.Embedding(input_dim=100, output_dim=5)\n",
        "\n",
        "embed_integers = embedding_layer(tf.constant([1, 2, 3]))\n",
        "\n",
        "embed_integers.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SA5YTIfQ1_1"
      },
      "source": [
        "### Define, Compile, and Train the Model\n",
        "\n",
        "Next, we will define a model that uses an `Embedding` layer to project the words in input sequences into 8-dimensional vectors. These vectors will be further processed through dense layers, and the last layer will predict the label of movie reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "PnBvxyc4UaIA"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 8\n",
        "\n",
        "# Create a model\n",
        "model = tf.keras.Sequential([\n",
        "       tf.keras.layers.Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=200),\n",
        "       tf.keras.layers.Flatten(),\n",
        "       tf.keras.layers.Dense(32, activation='relu'),\n",
        "       tf.keras.layers.Dropout(0.5),\n",
        "       tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3w5rY1jWc07"
      },
      "source": [
        "We will compile the model with `binary_crossentropy` loss (since there are only 2 labels: positive and negative review) and `adam` optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Pa9wGfvlWYtB"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4lU4IG4W1K6"
      },
      "source": [
        "Before training the model, we can see the model summary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNJhXZIMWzmZ",
        "outputId": "97d15d6e-f266-4547-823d-021cfe0a3d85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 200, 8)            160000    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1600)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                51232     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 32)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 211,265\n",
            "Trainable params: 211,265\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_um8S3dW4-j",
        "outputId": "e7d39768-f10b-4272-986f-7e40a45e79c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "782/782 [==============================] - 5s 5ms/step - loss: 0.4505 - accuracy: 0.7667 - val_loss: 0.2966 - val_accuracy: 0.8729\n",
            "Epoch 2/3\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.1925 - accuracy: 0.9297 - val_loss: 0.3146 - val_accuracy: 0.8685\n",
            "Epoch 3/3\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.0870 - accuracy: 0.9739 - val_loss: 0.3919 - val_accuracy: 0.8622\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_data, train_labels, validation_data = (val_data, val_labels), epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R45E02hfskwZ"
      },
      "source": [
        "<a name='section5-2'></a>\n",
        "\n",
        "## 18.5.2 Using TextVectorization Layer to Preprocess Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaIlzPVGh54c"
      },
      "source": [
        "Keras also provides another way to preprocess text and convert it to tokens, which is by using a TextVectorization layer. \n",
        "\n",
        "This layer performs the following preprocessing steps: \n",
        "* Standardize text by removing punctuations and lowering the text case.\n",
        "* Split sentences into individual words.\n",
        "* Convert the tokens into a numerical representation. \n",
        "\n",
        "The arguments in `TextVectorization` layer are:\n",
        "- *max_tokens*: maximum number of tokens in the vocabulary, where vocabulary is comprised of the unique words in the data. \n",
        "- *standardize*: denotes the standardization specifics to be applied to input data; by default, it is `lower_and_strip_punctuation' meaning to convert to lowercase and remove punctuations.\n",
        "- *split*: denotes what will be considered while splitting the input text; by default it is whitespace `\" \"`.\n",
        "- *output_sequence_length*: the length to which the sequences will be padded or truncated. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QfkdnmQkrSYw"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "x76D5avVlivO"
      },
      "outputs": [],
      "source": [
        "# Sample sentences\n",
        "sentences = ['TensorFlow is a deep learning library!',\n",
        "             'Is TensorFlow powered by Keras API?']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "7eYMFefqscFA"
      },
      "outputs": [],
      "source": [
        "text_vect_layer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=1000,\n",
        "    output_sequence_length=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaOetzBexcPA"
      },
      "source": [
        "We can use the `adapt()` method to fit the entences to the layer. The `adapt()` method will preprocess the data, and it will create a vocabulary that will be used later to convert text into a numerical representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "GdYJMqYGuf4L"
      },
      "outputs": [],
      "source": [
        "text_vect_layer.adapt(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXT4hQTQxiZP"
      },
      "source": [
        "Let's pass a sample sentence to inspect the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "WyQWnWL-xDV0"
      },
      "outputs": [],
      "source": [
        "sample_sentence = 'Tensorflow is a machine learning framework!'\n",
        "\n",
        "vectorized_sentence = text_vect_layer([sample_sentence])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Orginal sentence:', sample_sentence)\n",
        "print('Vectorized sentence:', vectorized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bnFLF8xZTC0",
        "outputId": "d7576020-6fd9-497a-db65-598d848ab68b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orginal sentence: Tensorflow is a machine learning framework!\n",
            "Vectorized sentence: tf.Tensor([[ 2  3 11  1  6  1  0  0  0  0]], shape=(1, 10), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No7WUUUhylIG"
      },
      "source": [
        "Since the words `'machine'` and `'framework'` were not part of the `sentences` that we passed to the layer, they are both represented by `1` in the vectorized output, since the index 1 is reserved for words that are out of vocabulary(`oov_token`).\n",
        "\n",
        "The output is padded with 0, and the length of the output sequence size is 10. \n",
        "\n",
        "The `TextVectorization` layer performed all required text preprocessing steps at once, and another advantage of this layer is that it can be used inside a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztGDcuYujNzp"
      },
      "source": [
        "<a name='section5-3'></a>\n",
        "\n",
        "## 18.5.3 Sequence Modeling with Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6ABwz_Am75n"
      },
      "source": [
        "***Recurrent Neural Networks (RNN)*** is a  neural network architecture that is designed for handling sequential data. Examples of sequential data are time-series, texts (sequence of words or characters), audio (sequence of sound waves), etc.\n",
        "\n",
        "Working with sequential data requires to preserve the sequence of the information flow in the data. For example, given the sentence `Today, I took my cat for a [....]`, to predict the next word, there should be a way to maintain the flow in the sequence of the words from the input to output. \n",
        "\n",
        "In conventional feedforward networks (such as networks composed of fully-connected or convolutional layers), the data flow from the input to the output. Conversely, in RNNs, there is a feedback loop at each time step, which creates the *recurrence*. This is shown in the next figure, where at each time step of the RNN model, an input (e.g., word) is processed, then in the next step the succeeding word is processed based on the information from the previous word, etc. This way, the network can learn dependencies between words that are not adjacent. \n",
        "\n",
        "<img src='https://raw.githubusercontent.com/avakanski/Fall-2022-Python-Programming-for-Data-Science/main/Lectures/Theme%203%20-%20Model%20Engineering%20Pipelines/Lecture%2018%20-%20Natural%20Language%20Processing/images/rnn.png' width=600px/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xBfAgEAzEm2"
      },
      "source": [
        "There are three major types of RNN layers: conventional (basic, simple, vanilla) RNN, LSTM (Long Short-Term Memory), and GRU (Gated Recurrent Units). They are implemented in Keras and PyTorch, and can be conveniently imported and used for creating models. in Keras, the conventional (i.e., basic) RNN is called SimpleRNN, and LSTM and GRU are called as they are written. \n",
        "\n",
        "While SimpleRNN has difficulty in handling long sequences, LSTM and GRU have the ability to store and preserve long-term dependencies over many time steps. Consequently, SimpleRNN are rarely used at present.\n",
        "\n",
        "Both LSTM and GRU layers use multiple gates to control to flow of information between the steps. For instance, LSTM layers include an input gate and an output gate to control the input and output information to each time step, a forget gate that removes irrelevant information, and a memory cell that saves important information. \n",
        "\n",
        "Next, we will apply an RNN model with LSTM layers for classification of text data. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyjIIjZzRlev"
      },
      "source": [
        "### Loading the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DAL7PtzV7gg"
      },
      "source": [
        "We are going to use the `ag_news_subset` dataset that is available in TensorFlow datasets. AG is a collection of news articles gathered from more than 2000 news sources. The news articles are classified into 4 classes: World(0), Sports(1), Business(2), and Sci/Tech(3). The total number of training samples is 120,000 and testing 7,600. \n",
        "\n",
        "Let's get the dataset from TensorFlow datasets. In the load function, `with_info=True` will return various information about the dataset (as shown in the next cells), and 'as_supervised=True` indicates that the data will be loaded as 2-element tuples consisting of (input, target) pairs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "U8fnaQT4CE9G"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "6JIKIZDRQwVV"
      },
      "outputs": [],
      "source": [
        "(train_data, val_data), info = tfds.load('ag_news_subset:1.0.0', #version 1.0.0\n",
        "                                         split=['train', 'test'],\n",
        "                                         with_info=True, \n",
        "                                         as_supervised=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "378IQW1eSVlz"
      },
      "source": [
        "We can use `info` to check basic information about the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03IBh4d1S1eJ",
        "outputId": "4c7112ab-759d-4c2d-dbce-53dfa6a25890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['World', 'Sports', 'Business', 'Sci/Tech']\n"
          ]
        }
      ],
      "source": [
        "# Displaying the classes\n",
        "class_names = info.features['label'].names\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUzqSrGwY7dU",
        "outputId": "5ede0cfd-6dd9-475e-9a06-821db2b7d0b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 120000\n",
            "Number of validation samples: 7600\n"
          ]
        }
      ],
      "source": [
        "print('Number of training samples:', info.splits['train'].num_examples) \n",
        "print('Number of validation samples:', info.splits['test'].num_examples) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AGay5wWZ_xs"
      },
      "source": [
        "We can use `tfds.as_dataframe` to display the first 10 news articles as  Pandas DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "kGj0TRaXaHG1",
        "outputId": "9e75890f-b3d1-4205-e34a-79db98eb457f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                         description  label\n",
              "0  b'AMD #39;s new dual-core Opteron chip is desi...      3\n",
              "1  b'Reuters - Major League Baseball\\\\Monday anno...      1\n",
              "2  b'President Bush #39;s  quot;revenue-neutral q...      2\n",
              "3  b'Britain will run out of leading scientists u...      3\n",
              "4  b'London, England (Sports Network) - England m...      1\n",
              "5  b'TOKYO - Sony Corp. is banking on the \\\\$3 bi...      0\n",
              "6  b'Giant pandas may well prefer bamboo to lapto...      3\n",
              "7  b'VILNIUS, Lithuania - Lithuania #39;s main pa...      0\n",
              "8  b'Witnesses in the trial of a US soldier charg...      0\n",
              "9  b'Dan Olsen of Ponte Vedra Beach, Fla., shot a...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6f092e78-36f2-4b47-bc86-ecb49ef103df\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_c29ac_\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th class=\"col_heading level0 col0\" >description</th>\n",
              "      <th class=\"col_heading level0 col1\" >label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_c29ac_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_c29ac_row0_col0\" class=\"data row0 col0\" >AMD #39;s new dual-core Opteron chip is designed mainly for corporate computing applications, including databases, Web services, and financial transactions.</td>\n",
              "      <td id=\"T_c29ac_row0_col1\" class=\"data row0 col1\" >3 (Sci/Tech)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c29ac_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_c29ac_row1_col0\" class=\"data row1 col0\" >Reuters - Major League Baseball\\Monday announced a decision on the appeal filed by Chicago Cubs\\pitcher Kerry Wood regarding a suspension stemming from an\\incident earlier this season.</td>\n",
              "      <td id=\"T_c29ac_row1_col1\" class=\"data row1 col1\" >1 (Sports)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c29ac_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_c29ac_row2_col0\" class=\"data row2 col0\" >President Bush #39;s quot;revenue-neutral quot; tax reform needs losers to balance its winners, and people claiming the federal deduction for state and local taxes may be in administration planners #39; sights, news reports say.</td>\n",
              "      <td id=\"T_c29ac_row2_col1\" class=\"data row2 col1\" >2 (Business)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c29ac_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_c29ac_row3_col0\" class=\"data row3 col0\" >Britain will run out of leading scientists unless science education is improved, says Professor Colin Pillinger.</td>\n",
              "      <td id=\"T_c29ac_row3_col1\" class=\"data row3 col1\" >3 (Sci/Tech)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c29ac_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_c29ac_row4_col0\" class=\"data row4 col0\" >London, England (Sports Network) - England midfielder Steven Gerrard injured his groin late in Thursday #39;s training session, but is hopeful he will be ready for Saturday #39;s World Cup qualifier against Austria.</td>\n",
              "      <td id=\"T_c29ac_row4_col1\" class=\"data row4 col1\" >1 (Sports)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c29ac_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
              "      <td id=\"T_c29ac_row5_col0\" class=\"data row5 col0\" >TOKYO - Sony Corp. is banking on the \\$3 billion deal to acquire Hollywood studio Metro-Goldwyn-Mayer Inc...</td>\n",
              "      <td id=\"T_c29ac_row5_col1\" class=\"data row5 col1\" >0 (World)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c29ac_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
              "      <td id=\"T_c29ac_row6_col0\" class=\"data row6 col0\" >Giant pandas may well prefer bamboo to laptops, but wireless technology is helping researchers in China in their efforts to protect the engandered animals living in the remote Wolong Nature Reserve.</td>\n",
              "      <td id=\"T_c29ac_row6_col1\" class=\"data row6 col1\" >3 (Sci/Tech)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c29ac_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
              "      <td id=\"T_c29ac_row7_col0\" class=\"data row7 col0\" >VILNIUS, Lithuania - Lithuania #39;s main parties formed an alliance to try to keep a Russian-born tycoon and his populist promises out of the government in Sunday #39;s second round of parliamentary elections in this Baltic country.</td>\n",
              "      <td id=\"T_c29ac_row7_col1\" class=\"data row7 col1\" >0 (World)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c29ac_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
              "      <td id=\"T_c29ac_row8_col0\" class=\"data row8 col0\" >Witnesses in the trial of a US soldier charged with abusing prisoners at Abu Ghraib have told the court that the CIA sometimes directed abuse and orders were received from military command to toughen interrogations.</td>\n",
              "      <td id=\"T_c29ac_row8_col1\" class=\"data row8 col1\" >0 (World)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_c29ac_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
              "      <td id=\"T_c29ac_row9_col0\" class=\"data row9 col0\" >Dan Olsen of Ponte Vedra Beach, Fla., shot a 7-under 65 Thursday to take a one-shot lead after two rounds of the PGA Tour qualifying tournament.</td>\n",
              "      <td id=\"T_c29ac_row9_col1\" class=\"data row9 col1\" >1 (Sports)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6f092e78-36f2-4b47-bc86-ecb49ef103df')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6f092e78-36f2-4b47-bc86-ecb49ef103df button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6f092e78-36f2-4b47-bc86-ecb49ef103df');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "news_df = tfds.as_dataframe(train_data.take(10), info)\n",
        "\n",
        "news_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksGa0Z4JaJDx"
      },
      "source": [
        "The columns in the DataFrame are `description` and `label`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omL7uadpazuc",
        "outputId": "3cb57db6-9ca3-4c6c-df0d-a79198ae13f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['description', 'label'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "news_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-jPA6TEd3sw"
      },
      "source": [
        "Now that we understand the data, let's prepare it before we can use LSTMs to classify the news.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKjyyw21bRzM"
      },
      "source": [
        "### Preparing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-KMfmo0dOWc"
      },
      "source": [
        "First, we will shuffle and batch the training data. For the validation data, we don't shuffle, we only batch it. \n",
        "\n",
        "The `buffer_size` below limits the number of data points to be shuffled to 1000. This can be useful when working with large datasets, that can not fit in the memory. \n",
        "\n",
        "The `prefetch()` function below is only added for optimizing the performance, and while the model is being trained, it will prefetch batches for validation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "i3e-BTCUfmdr"
      },
      "outputs": [],
      "source": [
        "buffer_size = 1000\n",
        "batch_size = 32\n",
        "\n",
        "train_data = train_data.shuffle(buffer_size)\n",
        "train_data = train_data.batch(batch_size).prefetch(1)\n",
        "val_data = val_data.batch(batch_size).prefetch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEIRO6ZlphLl"
      },
      "source": [
        "To convert the text data into tokens, in this case we will use the TextVectorizer layer in Keras. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 20000\n",
        "\n",
        "text_vectorizer = tf.keras.layers.TextVectorization(max_tokens=max_features)"
      ],
      "metadata": {
        "id": "tu8u1i612_sd"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will apply the `adapt()` method to preprocess the training data. Since the data was loaded as (input, label) tuples, the `lambda` function applies the vectorizer only on the input features (in the column `description`), and not on the `labels`. The vectorization is applied only to the input text, and not to the labels.  "
      ],
      "metadata": {
        "id": "JGW-6Q7YXYUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorizer.adapt(train_data.map(lambda description, label: description))"
      ],
      "metadata": {
        "id": "4RDwZoxVG5Np"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfiumAc0rriT"
      },
      "source": [
        "Let's pass two new sentences to `text_vectorizer`. The vectorized sequences will be padded with the maximum sentences, but if we want to have fixed size, we can set the `output_sequence_length` to another value in the layer initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Hb4d3EqrrMuU"
      },
      "outputs": [],
      "source": [
        "sample_news = ['This weekend there is a sport match between Man U and Fc Barcelona',\n",
        "               'Tesla has unveiled its humanoid robot that appeared dancing during the show!']             "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7cQZbSIsjhA",
        "outputId": "9bdb324e-e1ee-4d31-8a13-70f0dc354627"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   40,   491,   185,    16,     3,  1559,   560,   163,   362,\n",
              "        13418,     7,  7381,  2517],\n",
              "       [    1,    20,   878,    14,     1,  4663,    10,  1249, 11657,\n",
              "          159,     2,   541,     0]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "vectorized_news = text_vectorizer(sample_news)\n",
        "vectorized_news.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDiIP1tFtRYs"
      },
      "source": [
        "Note that the second sentence was padded with 0. Also the words `Tesla` and `humanoid` have an index of 1 because they were not a part of the training data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUNqJkjpusFA"
      },
      "source": [
        "### Creating and Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7Vobpziu4Zw"
      },
      "source": [
        "We are going to create a Keras model that takes the tokenized text as input and outputs the class of the news articles.\n",
        "\n",
        "The model has the following layers:\n",
        "\n",
        "* `TextVectorization layer` for converting input texts into tokens.\n",
        "* `Embedding layer` for representing the tokens with a trainable feature vector. Because the feature vector is trainable, the words that have similar semantic meanings will be close feature vectors in the embedding space. \n",
        "* `LSTMs layer` for processing the sequences. The layer is wrapped into a Bidirectional layer, which will process the sequences from both directions (forward and backward), i.e., one LSTM layer will process the sequences forward, another layer will process the sequences backward, and the outputs of the two LSTMs will be combined. \n",
        "* `Dense layer` for classification purpose. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnRD1ymezwok",
        "outputId": "f7cfc9c7-1f91-436f-c06f-c53e2b035ec5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "input_dim = len(text_vectorizer.get_vocabulary())\n",
        "input_dim "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "ApxFc7DouIAX"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    text_vectorizer,\n",
        "    tf.keras.layers.Embedding(input_dim=input_dim, output_dim=64, mask_zero=True),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "E3dtgRIH1BL_"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbJ0GXUi1ToF",
        "outputId": "a8424a21-6ba9-475a-f7b5-f2c7c9cf71e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "3750/3750 [==============================] - 66s 16ms/step - loss: 0.3301 - accuracy: 0.8845 - val_loss: 0.2733 - val_accuracy: 0.9079\n",
            "Epoch 2/5\n",
            "3750/3750 [==============================] - 54s 14ms/step - loss: 0.2015 - accuracy: 0.9298 - val_loss: 0.2817 - val_accuracy: 0.9083\n",
            "Epoch 3/5\n",
            "3750/3750 [==============================] - 54s 14ms/step - loss: 0.1346 - accuracy: 0.9522 - val_loss: 0.3575 - val_accuracy: 0.9067\n",
            "Epoch 4/5\n",
            "3750/3750 [==============================] - 54s 14ms/step - loss: 0.0852 - accuracy: 0.9691 - val_loss: 0.4135 - val_accuracy: 0.8997\n",
            "Epoch 5/5\n",
            "3750/3750 [==============================] - 57s 15ms/step - loss: 0.0540 - accuracy: 0.9805 - val_loss: 0.5116 - val_accuracy: 0.8959\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "history = model.fit(train_data, epochs=5, validation_data=val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7ZdyxK77qfW",
        "outputId": "4a30a4fc-05df-4121-acb0-299ee9defd74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "[[0.00155084 0.9903066  0.00352476 0.00461771]]\n"
          ]
        }
      ],
      "source": [
        "# Predicting the class of new news articles\n",
        "sample_news_1 = ['The self driving car company Tesla has unveiled its humanoid robot that appeared dancing during the show!']\n",
        "\n",
        "# make predictions on the sample_news 1\n",
        "predictions_1 = model.predict(sample_news_1)\n",
        "print(predictions_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdAqAXUzN2Xe"
      },
      "source": [
        "The model correctly predicted that the news article is related to tech or science. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# find the index of the predicted class\n",
        "predicted_class_1 = np.argmax(predictions_1)\n",
        "\n",
        "print('Predicted class:', predicted_class_1)\n",
        "print('Predicted class name:', class_names[predicted_class_1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiHGFA0seiAD",
        "outputId": "a0bdcc04-8587-4670-f5fb-e2a953a71d04"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 1\n",
            "Predicted class name: Sports\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKxv86w5OWhG"
      },
      "source": [
        "One more example is provided in the next cell. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDXFamqkOHAD",
        "outputId": "352070ec-7194-4487-edf4-1e36658803d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 29ms/step\n",
            "Predicted class: 1\n",
            "Predicted class name: Sports\n"
          ]
        }
      ],
      "source": [
        "# Predicting the class of new news\n",
        "sample_news_2 = ['This weekend there is a match between two big footbal teams in the national league']\n",
        "\n",
        "predictions_2 = model.predict(sample_news_2)\n",
        "\n",
        "predicted_class_2 = np.argmax(predictions_2)\n",
        "\n",
        "print('Predicted class:', predicted_class_2)\n",
        "print('Predicted class name:', class_names[predicted_class_2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vweobvFVe4RB"
      },
      "source": [
        "<a name='section6'></a>\n",
        "\n",
        "# References\n",
        "\n",
        "1. Complete Machine Learning Package, Jean de Dieu Nyandwi, available at: [https://github.com/Nyandwi/machine_learning_complete](https://github.com/Nyandwi/machine_learning_complete).\n",
        "2. Deep Learning with Python, Francois Chollet, Second Edition, Manning Publications, 2021.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDbzcXmWe4RB"
      },
      "source": [
        "[BACK TO TOP](#section0)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}